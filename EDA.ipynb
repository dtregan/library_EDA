{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (dtregan - uploaded 14/02/2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. \"Write code to produce a report of the frequency with which books have been loaned in reverse order (the least frequent first). The report should indicate the number of times that books were loaned in 2019, along with their titles and author. The latter are required as there are some texts (classics in some way) which will be retained even if they have proved unpopular.\"\n",
    "\n",
    "2. \"Write code to produce a report of the most popular genres and sub-genres of books borrowed. This will inform the library as to the interests of its readers and therefore may influence future purchasing decisions.\"\n",
    "\n",
    "3. \"Write code to produce a report of the average length of time that a user has a book on loan and the proportion of users who have returned books late, together with the average late period.\"\n",
    "\n",
    "\n",
    "## Preamble: Project decomposition\n",
    "\n",
    "In terms of approaching this assignment, I split the 'problem' into three distinct areas (although it ends up only using two larger sets of code, given there being some eventual crossover in using the same dataset (looking for different outcomes) for Tasks 1 and 2.\n",
    "\n",
    "I felt as though the primary data types that would be useful to utilise would be:\n",
    "\n",
    "- list: mainly for their flexibility and in terms of methods\n",
    "- dictionary: mainly for the ability to have a (common) key which could amass useful information from both csv-read files\n",
    "- Task 1 in particular converts lists into dictionaries, and then back into lists. The 'amassing' function that dictionaries have i.e. linking a lot of list-of-list-like details into a key:value partnership made it quite straightforward to use information from both .csv files.\n",
    "\n",
    "Sets and tuples were also used briefly. I found list (and rarely, dictionary) comprehension to be an invaluable tool for working through and iterating through elements of lists (/dictionaries). \n",
    "\n",
    "It was extremely useful to be able to amass information under 'book_id', with data taken from both of the files read using the csv module and most importantly using dictionaries as the tool to amass this information; e.g. book ID no. 1 as the key could gather the information (as values) from both read csv files, after a bit of data format conversion and changing the Epoch date format into smaller numbers (for task 3) or countable data (for task 1).\n",
    "\n",
    "In terms of de-composing the project, I was able to see that:\n",
    "\n",
    "- Task 1 would utilise both csv files\n",
    "- Task 2 would utilise just 'books.csv'\n",
    "- Task 3 would utilise both csv files.\n",
    "\n",
    "Therefore, being able to get book_loans.csv cleaned up at the start was useful as it could be referred to and used through the program.\n",
    "\n",
    "The main aims for the tasks, and in terms of decomposing the project, were:\n",
    "\n",
    "- cleaning the data i.e. removing post-ID120 entries on 'book_loans.csv', getting either the small two-digit (or 0) loan periods or year figures from the Microsoft Excel epoch figures\n",
    "- ensuring that I could marry the information two csv files together for tasks 1 and 3, and by a common key of the book_id\n",
    "- ensuring that it was easy to find out where any errors or disruptive code might be by using good naming practices for variables\n",
    "- ensuring that future datasets could be analysed with a minimal amount of program tinkering\n",
    "- use appropriate functions which could be re-referred to if needs be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task breakdown and considerations\n",
    "\n",
    "Given the nature producing 'reports' from each task, as outlined in the assignment, I thought the best way to represent this would be to write (print) statements to a file created (in 'w' mode) at the end of the file, with code preceding it which would get it into the correct format and ensure that the outputs in the created file are worthwhile and easy to read/understand.\n",
    "\n",
    "I was conscious that editing the read data (defined as 'books' and 'book_loans') would have made problems for me later on; thus, I made a copy using the slice notation method of doing so ([:]).\n",
    "\n",
    "Prep\n",
    "- Data cleaning is required - primarily around removing the entries from the variable-defined 'bookloans.csv' copy (book_loans) where the book ID is greater than 120, as there are no book records for these, and the task brief outlined that I should remove them. This new dataset (amended_list_cl) is then able to be used and referenced for Tasks 1 and 3.\n",
    "\n",
    "__Task 1__<br>\n",
    "The output for this task was required to be:\n",
    "- frequency with which books have been loaned in reverse order (the least frequent first)\n",
    "- indication of no. of times that books were loaned in 2019, along with their titles and author\n",
    "\n",
    "Most importantly was the need to clean the data to be able to get it into a usable form. The steps to get the output were:\n",
    "- define the encoding in opening 'bookloans.csv' as 'uf-8-sig'; the first index in the first line returned 'ufeff' otherwise.\n",
    "\n",
    "- ascertaining whether the books were borrowed in 2019 or not was the next step; you can see the working out in the code, but essentially, each Epoch date value was divided by 365 (days in a year) then checked to see if that figure was within 119 and 120 (119 = 2019 and 120 = 2020 i.e. 119 years since the starting Epoch date). If so, a string ('Y_2019' or 'N_2019') was appended to a list. Values of 0 (not returned) were ignored, as they weren't returned so year is irrelevant. The 'Y_2019'/'N_2019' values were then appended to another list with their corresponding ID values via pulling that information from the original cleaned list of data and both the enumerate() function and list comprehension. <br>\n",
    "<br>__NOTE__: Picking 'Y_2019' and 'N_2019' was due to book titles being very unlikely in having this information; however, to ensure that later on, when these values were counted within a set of dictionary values, I could have used something even more obscure e.g. random characters for each.<br><br>\n",
    "- a dictionary (d1) was then created to pull the values together under their first value (the book ID being the key, and 'Y_2019'/'N_2019' being the values).\n",
    "- For getting the book details from book.csv, firstly, I created a file which got rid of the header (column titles) using slice notation ([1:]). \n",
    "- I used dictionary comprehension to create a dictionary (d2) from the values in each line; the key would be the book_ID, with the values being the book details e.g. title, author, genre, subgenre (the latter two useful for Task 2).\n",
    "- I then combined both d1 and d2 together with the key of book_ID; this resulting dictionary (result_dict) was then converted into a list of lists and then sorted (using items() and list() as well as sorted()).\n",
    "- A new list was then created which was appended with all of the verbose book details (ID, title, author, genre, subgenre) as well as three counts: how many values of 'Y_2019', of 'N_2019', and both together. These would give you: loans in 2019, loans not in 2019, and total loans.\n",
    "- The data could then be used to make conclusions from (see the Report Generation section at the end of the code).\n",
    "\n",
    "__Task 2__<br>\n",
    "The output for this task was required to be:\n",
    "- most popular genres\n",
    "- most popular subgenres\n",
    "\n",
    "This was a little vague, so I decided to output both the outright most popular genre and subgenre, and then rank the genres/subgenres afterwards by loan amount. I got to this stage by:\n",
    "\n",
    "- adding the genre/subgenre information to a seperate list via appending in a for loop\n",
    "- flattening the list of lists to make it easier to utilise\n",
    "- utilising list comprehension and converting the lists to sets (as well as using .count()) to get totals for each\n",
    "- sorting the genres/subgenres into high-low via .sorted() and a lambda function so that the data is in the correct order.\n",
    "\n",
    "__Task 3__<br>\n",
    "The output for this task was required to be:\n",
    "- report the average length of time that a user has a book on loan\n",
    "- the proportion of users who have returned books late\n",
    "- average late period\n",
    "\n",
    "The steps I took to get the data for the report were:<br>\n",
    "(Task 3a)\n",
    "- I started out similar to Task 1 for this - except I realised that I didn't need book.csv, only bookloans.csv.\n",
    "- I imported 'bookloans.csv' at the start as bookloans.csv and was able to revisit this, without the post-120 book IDs, as 'amended_list_cl'.\n",
    "- I was aware that using the value of 0 of date_of_return would potentially be a problem, so I created another list and appended the lines/rows where the value of date_of_return, if 0, would instead be appended as 'Not returned'. This would work as a kind of error handling in itself, with the data being in str format rather than int.\n",
    "- using the Epoch data format felt clunky, so I created/appended to a list of data which contained the subtracted larger amount (the second, date_of_return) from the smaller amount (the first, date_of_loan), to give you the correct smaller (loan period in days) figure. If the second value was 0/'Not returned', it wasn't counted (for the first part of the task (average length of time that a user has a book on loan)).\n",
    "- Similarly to Task 1, the just-calculated loan length values were then appended to another list with their corresponding member_ID values via pulling that information from the original cleaned list of data and both the enumerate() function and list comprehension. They were then fed into a dictionary with the key as the member id, which meant that the member IDs (as keys) had a series of associated values which were the length of each of the loans that that membmer had, in days.\n",
    "- To get the data for the report, I then made another dictionary which had the keys as the member ID, and the values as the avg of the associated values with that key. The overall average was worked out by finding the average of all of the values of the values in that newer dictionary (avg_len_dict). Also, due to the slightly ambiguous wording of the question, I returned the values for each of the users, in ID order.\n",
    "\n",
    "(Task 3b)\n",
    "- I put together a function much the same as the previous sub-task (3a), except that a crucial bit of code counted the overall number of members via committing the list created to a dictionary and then calculating the overall number of users by using both len() and the amount of keys (via .keys()) in the users dictionary.\n",
    "- The next steps use list comprehension to filter the values - if they are over 14 (>14), then they are then added to newly-created lists (one for counting non-returned books as late returns, and the other not counting them); each of these is then added to a dictionary which is then analysed (via dictionary.keys()) to see how many keys there are present i.e. how many users have had a late return on their records.\n",
    "- The values are then returned - percentages are calculated via the usual fashion (amount of late users / overall amount of users * 100).\n",
    "\n",
    "(Task 3c)\n",
    "- This subtask uses the following code to add all of the loan periods that were post-14, or overdue: the overall average is then calculated in the report in both an overall >14 format, but also as an average of the days beyond the 14 day limit.\n",
    "\n",
    "#### Testing & 'sanity checks'\n",
    "\n",
    "While putting together the code, I used the following methods to ensure that the data was correct at various stages:\n",
    "\n",
    "- printing halfway through statements e.g. putting a print inside a for loop to ensure that the output was correct. A few of these have been left in to show where they would've been when testing the overall output. If the results didn't seem to be correct, utilising these print statements as checkpoints was useful to ensure that I knew (roughly) where the error was.\n",
    "- utilising the in-built printed statements (information from the functions etc.) prior to putting together the report generation code at the end. Again, these are commented-out to make the output cleaner, but these were very useful to get the report at the end in the eventual correct format.\n",
    "- using try & except on the files to ensure that errors are correctly handled.\n",
    "- when importing 'bookloans.csv', 'encoding' within the .open() function needed to be specified as 'utf-8-sig', or it would replace the first index in the first line as 'ufeff'; this would've caused all sorts of issues with data integrity and checking further down the line. The other .csv didn't have that issue.\n",
    "- making sure in the book_details() function that I had books_list = books[1:]; this eliminates the header in row 0 + allows me to safely edit the rest of the list and avoid any errors with using the column titles rather than the proper data.\n",
    "- using 'with open() as var' when reading and write both the .csv files and the .txt file (at the end) means that the files are closed once they have been read/written, (hopefully) avoiding any potential file-related errors.\n",
    "- Using a string of 'Not returned' as a stand-in figure for integer 0 where 'date_of_return' was used was chosen in case the figure of 0 interfered with any averages by accident.\n",
    "\n",
    "See Appendix 1 for a brief review of the quality of the data used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # imports the two files needed for the exercise\n",
    "\n",
    "try: # error handling - finally not used as .csv reading is essential\n",
    "    with open('books.csv','r') as file:\n",
    "        books = csv.reader(file) # only returns an object, see next line\n",
    "        books = list(books) # converts into list format which can then be indexed/used/read etc.\n",
    "        \n",
    "    with open('bookloans.csv','r',encoding='utf-8-sig') as file: # get a 'ufeff' character in first line otherwise\n",
    "        book_loans = csv.reader(file)\n",
    "        book_loans = list(book_loans)\n",
    "\n",
    "except:\n",
    "     print(\"The files 'books.csv' and 'bookloans.csv' are not in the same enclosing folder as this .ipynb file. Please amend and run the program again.\")\n",
    "\n",
    "amended_list = book_loans[:] # create a duplicate list that I can safely edit and use throughout\n",
    "\n",
    "def data_clean():\n",
    "    \"\"\"\n",
    "    Removes all book_ids which are greater than 120; the project brief specified as such.\n",
    "    Returns the same dataset as bookloans.csv but without those IDs.\n",
    "    \"\"\"\n",
    "    amended_list_clean = []\n",
    "    for line in amended_list:\n",
    "        if int(line[0]) > 120: # eliminates records where ID is greater than 120 (no book information exists for these)\n",
    "            pass\n",
    "        else:\n",
    "            amended_list_clean.append(line)\n",
    "    return amended_list_clean\n",
    "\n",
    "amended_list_cl = data_clean() # can be used going forward for tasks 1 & 3\n",
    "#print(amended_list_cl) - checks the format of the 'cleaned' dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_2019():\n",
    "    \"\"\"\n",
    "    Functon used to make a dictionary which contains the 'book_id's as keys and instances of 'Y_2019' \n",
    "    (yes to loaned in 2019) or 'N_2019' (no to loaned in 2019) which can later be counted to provide\n",
    "    overall counts regarding amount of loans per book.\n",
    "    \n",
    "    Don't need the member ID for Task 1 and Task 2 so removed at this stage.\n",
    "    \n",
    "    NOTE:\n",
    "    Only the date_of_return can ever be 0, so [3](date_of_return index in this instace) is ample for checking\n",
    "    \"\"\"\n",
    "    loaned_list = []\n",
    "    \n",
    "    for line in amended_list_cl:\n",
    "        if line[3] == 0: # 0 = not returned, so date_of_return value ignore. date_of_loan still utilised\n",
    "            if ((int(line[2])/365 < 119) or (int(line[2])/365 >= 120)):\n",
    "                loaned_list.append('N_2019')\n",
    "            else:\n",
    "                loaned_list.append('Y_2019')\n",
    "        else:\n",
    "            if ((int(line[2])/365 < 119) or (int(line[2])/365 >= 120)) or ((int(line[3])/365 < 119) or (int(line[3])/365 >= 120)):\n",
    "                loaned_list.append('N_2019')\n",
    "            else:\n",
    "                loaned_list.append('Y_2019')\n",
    "                \n",
    "    amended_list_books = [l[0:1] + [loaned_list[i]] for i, l in enumerate(amended_list_cl)] # don't require member id\n",
    "    \n",
    "    d = {} # frequency of books loaned, 'N_2019's and 'Y_2019's can be counted\n",
    "    for k,v in amended_list_books:\n",
    "            d.setdefault(k,[]).append(v) # book_id is k, N_2019s/Y_2019s are values\n",
    "    \n",
    "    return d\n",
    "\n",
    "d1 = div_2019()\n",
    "# print(d1) - another print check\n",
    "\n",
    "def book_details():\n",
    "    \"\"\"\n",
    "    This function builds a new dictionary which contains book_id as key, and book details (book_id,title,author,\n",
    "    genre,subgenre) as values. \n",
    "    \n",
    "    Dictionary comprehension is used to convert a list contain book titles into a dictionary.\n",
    "    \n",
    "    Publisher isn't required so is not used.\n",
    "    \"\"\"\n",
    "    new_books= []\n",
    "    book_freq = {}\n",
    "    books_list = books[1:] # eliminates header in row 0 + allows me to safely edit\n",
    "    for line in books_list:\n",
    "        new_books.append(line[0:5])\n",
    "    \n",
    "    book_freq = {item[0]: item[1:] for item in new_books} # dict comprehension to create a dict from book details\n",
    "    \n",
    "    return book_freq\n",
    "\n",
    "d2 = book_details()\n",
    "# print(d2) - another print check\n",
    "dicts = [d2,d1] # both dictionaries are combined together\n",
    "# print(dicts) # another print check\n",
    "result_dict = {}\n",
    "\n",
    "for d in dicts: # combines the two dictionaries together into one, using the book_id as the common value\n",
    "    for k, v in d.items():\n",
    "        result_dict.setdefault(k, []).extend(v)\n",
    "        \n",
    "count_result_dict = (dict(sorted(d.items(),key = lambda x: x[1],reverse=False))) # sorts the above dictionary\n",
    "result_dict = list(result_dict.items()) # converts the items (both keys and values) in result_dict dictionary into a list\n",
    "\n",
    "def total_loan_details():\n",
    "    \"\"\"\n",
    "    Converts the list (result_dict) into another usable list from which details can be referenced from in the report.\n",
    "    \"\"\"\n",
    "    loans_total_info = []\n",
    "    index = 0\n",
    "    for line in result_dict:\n",
    "        loans_2019 = line[1].count('Y_2019')\n",
    "        loans_not_2019 = line[1].count('N_2019')\n",
    "        loans_total_info.append([result_dict[index][0],result_dict[index][1][0:1],result_dict[index][1][1:2],result_dict[index][1][2:3],result_dict[index][1][3:4],loans_2019,loans_not_2019,loans_2019 + loans_not_2019])\n",
    "        index += 1\n",
    "    return loans_total_info\n",
    "    \n",
    "cleaned_data = total_loan_details() # Columns are: ID, TITLE, AUTHOR, GENRE, SUBGENRE, 2019 LOANS, NOT 2019 LOANS, TOTAL LOANS\n",
    "sorted_clean_data = sorted(cleaned_data,key=lambda x:x[7]) # sorted data from low to high from values in index [7] i.e. total loans\n",
    "# print(cleaned_data) - two print checks to check that what I want to happen has happened\n",
    "# print(sorted_clean_data)\n",
    "\n",
    "# for line in sorted_clean_data: \n",
    "#     print(f\"Book ID: {line[0]}; {line[2]} (AUTHOR); '{(line[1])}' (TITLE); Total loan frequency: {line[7]}; Total loan frequency in 2019: {line[5]}.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_genre():\n",
    "    \"\"\"\n",
    "    Separate lists are created which pull the genres and subgenres from 'sorted_clean_data', completed\n",
    "    for Task 1. \n",
    "    \"\"\"\n",
    "    genres = []\n",
    "    subgenres= []\n",
    "    for line in sorted_clean_data:\n",
    "        genres.append(line[3])\n",
    "        subgenres.append(line[4])\n",
    "    \n",
    "    return genres, subgenres\n",
    "\n",
    "genres = pop_genre()[0] # 0 = return of genres\n",
    "subgenres = pop_genre()[1] # 1 = return of subgenres\n",
    "\n",
    "genres_flat = [item for sublist in genres for item in sublist] # flattens the list of lists i.e. pulling the elements out of []\n",
    "subgenres_flat = [item for sublist in subgenres for item in sublist] # same as genres_flat but for subgenres\n",
    "# print(genres_flat) - print check\n",
    "# print(subgenres_flat) - print check\n",
    "\n",
    "coll_genres = [[x,genres_flat.count(x)] for x in set(genres_flat)] # counts the instances of each element and returns the element & count\n",
    "coll_subgenres = [[x,subgenres_flat.count(x)] for x in set(subgenres_flat)] # counts the instances of each element and returns the element & count\n",
    "# print(coll_genres) - print check\n",
    "# print(coll_subgenres) - print check\n",
    "\n",
    "sorted_genres = sorted(coll_genres,key=lambda x:x[1],reverse=True) # sorts coll_genres from high to low\n",
    "sorted_subgenres = sorted(coll_subgenres,key=lambda x:x[1],reverse=True) # sorts coll_subgenres from high to low\n",
    "\n",
    "# print(sorted_genres) - print check\n",
    "# print(sorted_subgenres) - print check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a.\n",
    "def avg_len_loan():\n",
    "    \"\"\"\n",
    "    This function calculates the average loan length (and average per person) by:\n",
    "    \n",
    "    - creating a new column of data which is date_of_return - date_of_loan (i.e. loan period in days)\n",
    "    - adds this new column of data to a new list (amended_list_avgloan) which also contains member_id\n",
    "    - creating a new dictionary which has member_id as key and associated loan period totals as values\n",
    "    - creating another dictionary which averages we then calculated from the values of each key, then returned\n",
    "    - \n",
    "    \n",
    "    NOTE:\n",
    "    # where date_of_return == '0' i.e. ('not returned') ->  not counted in loan period average\n",
    "    # 'Not returned' was chosen in case the figure of 0 interfered with any averages by accident\n",
    "    \"\"\"\n",
    "    loan_list = []\n",
    "  \n",
    "    for line in amended_list_cl:\n",
    "        if int(line[2]) > 0 and int(line[3]) > 0: \n",
    "            loantime = (int(line[3]) - int(line[2])) # date_of_return - date_of_loan\n",
    "            loan_list.append(loantime)\n",
    "        else: \n",
    "            loan_list.append('Not returned') # for values of 0\n",
    "        \n",
    "    amended_list_avgloan = [l[1:2] + [loan_list[i]] for i, l in enumerate(amended_list_cl)] # don't require book_id (line item[0]) for this exercise\n",
    "     \n",
    "    d = {}\n",
    "    for k,v in amended_list_avgloan:\n",
    "        if v == 'Not returned': \n",
    "            pass # i.e. ignored\n",
    "        else:\n",
    "            d.setdefault(k,[]).append(v)\n",
    "            \n",
    "    avg_len_dict = {}\n",
    "    for k,v in d.items():\n",
    "        avg_len_dict[k] = sum(v)/ (len(v))\n",
    "    \n",
    "    overall_avg = sum(avg_len_dict.values()) / len(avg_len_dict.values())\n",
    "    \n",
    "    return avg_len_dict, overall_avg\n",
    "    \n",
    "avglenloan = avg_len_loan()\n",
    "# print(avglenloan) - print check\n",
    "  \n",
    "# 3b)\n",
    "# overall amount of members\n",
    "\n",
    "def late_users():\n",
    "    \"\"\"\n",
    "    This function calculates the proportion of late users by:\n",
    "    \n",
    "    - creating a new column of data which is date_of_return - date_of_loan (i.e. loan period in days)\n",
    "    - adds this new column of data to a new list (late_list_cl) which also contains member_id\n",
    "    - akin to the previous function, creating a dictionary where member_id is the key and loan periods are values\n",
    "    - getting member count; calculated by finding the length of the amount of members\n",
    "    - (for both late_users_most & late_users_all) producing new dictionaries which contain the keys as the \n",
    "    member ids but only the values higher than 14 (>14) i.e. late returns (as well as == 0 for the special\n",
    "    case). The length each of these dictionaries is then returned, and compared to the total amount of members;\n",
    "    which returns proportions in the report.\n",
    "    \n",
    "    NOTE:\n",
    "    # Two totals for 'late returns' are returned: one where non-returns are included, and another where \n",
    "    they aren't. Comments below make this clear.\n",
    "    \"\"\"\n",
    "    users = {}\n",
    "    loan_list = []\n",
    "    \n",
    "# total members\n",
    "    for line in amended_list_cl:\n",
    "        if int(line[2]) > 0 and int(line[3]) > 0: # day of return - day taken out\n",
    "            loantime = (int(line[3]) - int(line[2]))\n",
    "            loan_list.append(loantime)\n",
    "        else: \n",
    "            loan_list.append('Not returned')\n",
    "  \n",
    "    late_list_cl = [l[1:2] + [loan_list[i]] for i, l in enumerate(amended_list_cl)] # don't require book_id (line item[0]) for this exercise\n",
    "    \n",
    "    for k,v in late_list_cl:\n",
    "            users.setdefault(k,[]).append(v)\n",
    "    user_count = len(users.keys())\n",
    "\n",
    "# returned books late - not counting users who haven't returned at all\n",
    "\n",
    "    new_lst = [x for x in late_list_cl if x[1] != 'Not returned']\n",
    "    new_lst_late = [x for x in new_lst if x[1] > 14]\n",
    "    d_late_most = {}\n",
    "    for k,v in new_lst_late:\n",
    "        if v == 'Not returned':\n",
    "            pass # ignored\n",
    "        else:\n",
    "            d_late_most.setdefault(k,[]).append(v)\n",
    "    late_users_most = len(d_late_most.keys())\n",
    "\n",
    "# returned books late - counting users who haven't returned at all\n",
    "\n",
    "    new_lst_2 = [x for x in late_list_cl if x[1] == 'Not returned' or x[1] > 14]\n",
    "    d_late_all = {}\n",
    "    for k,v in new_lst_2:\n",
    "            d_late_all.setdefault(k,[]).append(v)\n",
    "    late_users_all = len(d_late_all.keys())\n",
    "    \n",
    "    return (late_users_most / user_count * 100), (late_users_all / user_count * 100), late_users_most, late_users_all, user_count\n",
    "\n",
    "prop_late = late_users()\n",
    "# print(prop_late) - print check\n",
    "\n",
    "# 3c)\n",
    "def avg_late_loan():\n",
    "    \"\"\"\n",
    "    This small function does much the same as avg_len_loan(), except only takes counts all of those loan periods\n",
    "    which are greater than 14. An average is found by appending those values to avg_loan_list and then using\n",
    "    sum() and len() to find an average.\n",
    "    \"\"\"\n",
    "    avg_loan_list = []\n",
    "    \n",
    "    for line in amended_list_cl:\n",
    "        if int(line[3]) > 0: # date_of_return is 0 only if 'not returned'\n",
    "            loantime = (int(line[3]) - int(line[2])) # date_of_return - date_of_return\n",
    "            if loantime > 14:\n",
    "                avg_loan_list.append(loantime)\n",
    "    return (sum(avg_loan_list) / len(avg_loan_list))\n",
    "\n",
    "avglateloan = avg_late_loan()\n",
    "# print(avglateloan) - print check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report file written successfully! See the contents of the file named 'books_tasks_report.txt' for a breakdown of the stats associated with the tasks.\n"
     ]
    }
   ],
   "source": [
    "def report_generate():\n",
    "    \"\"\"\n",
    "    This function presents the report which could be handed to whoever to provide the statistics/figures required.\n",
    "\n",
    "    f.write() commands, some inside for loops for iteration, generate the content and structure of the report.\n",
    "\n",
    "    Using 'with open() as var' means that I don't need to use close().\n",
    "\n",
    "    Try and except blocks manage any error handling in case there are any issues.\n",
    "\n",
    "    NOTE:\n",
    "    # Task 3 - the average loan period for each user is also returned as the original task question is unclear.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('books_tasks_report.txt','w',encoding='utf-8') as f:\n",
    "            f.write('TASK 1\\n')\n",
    "            f.write('Book data sorted below in terms of frequency with which books have loaned (in reverse order; least frequent first):\\n')\n",
    "            f.write('(NOTE: Stat also present which indicates how many times each particular book was loaned in 2019)\\n')\n",
    "            f.write('\\n')\n",
    "            for line in sorted_clean_data:\n",
    "                f.write(f\"Book ID: {line[0]}; {line[2]} (AUTHOR); '{(line[1])}' (TITLE); Total loan frequency: {line[7]}; Total loan frequency in 2019: {line[5]}.\\n\")\n",
    "            f.write('\\n')\n",
    "            f.write('TASK 2\\n')\n",
    "            f.write('\\n')\n",
    "            f.write(f\"Most popular genre: {sorted_genres[0][0]}. Most popular subgenre: {sorted_subgenres[0][0]}.\\n\")\n",
    "            f.write('\\n')\n",
    "            f.write('Most popular genres ranked in terms of loans (ranked highest to lowest):\\n')\n",
    "            for entry in sorted_genres:\n",
    "                f.write(f\"Genre: {entry[0]}. Loans: {entry[1]}\\n\")\n",
    "            f.write('\\n')\n",
    "            f.write('Most popular subgenres ranked in terms of loans (ranked highest to lowest):\\n')\n",
    "            for entry in sorted_subgenres:\n",
    "                f.write(f\"Subgenre: {entry[0]}. Loans: {entry[1]}\\n\")\n",
    "            f.write('\\n')\n",
    "            f.write('TASK 3\\n')\n",
    "            f.write('\\n')\n",
    "            f.write(f\"Average length of loan overall: {round(avglenloan[1],2)} days.\\n\")\n",
    "            f.write('\\n')\n",
    "            f.write(\"Average length of loan, per user:\\n\")\n",
    "            for k, v in sorted(avglenloan[0].items(), key = lambda item: int(item[0])):\n",
    "                f.write(f\"ID: {k}. Avg. loan duration: {round(v,2)} days\\n\")\n",
    "            f.write('\\n')\n",
    "            f.write(f\"Proportion of users who returned at least one book late (not including instances where books weren't actually returned): {prop_late[0]}%, or {prop_late[2]} out of {prop_late[4]} users.\\n\")\n",
    "            f.write(f\"If instances where books weren't returned are included as late returns: {prop_late[1]}%, or {prop_late[3]} out of {prop_late[4]} users.\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Average length of late loans: {round(avglateloan,2)} days, or an average of {round(avglateloan-14,2)} days beyond the accepted 14-day loan period.\")\n",
    "        print(\"Report file written successfully! See the contents of the file named 'books_tasks_report.txt' for a breakdown of the stats associated with the tasks.\")\n",
    "\n",
    "    except:\n",
    "        print(\"Report file not written successfully. Please check your file path or contact the program author for troubleshooting.\")\n",
    "        \n",
    "report_generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: Report on data quality\n",
    "\n",
    "I have highlighted below in terms of my thoughts of the data provided in the .csv files:\n",
    "\n",
    "- the fact that there was 140 different book IDs but only 120 titles in'books.csv' data. Although this was noted in the 'The Problem' section, it is still frustrating to work with data that is lopsided i.e. you will need to remove data from 'bookloans.csv' even though it would potentially be useful for reporting.\n",
    "\n",
    "- no title header in bookloans.csv, but present in books.csv. Awkward for two reasons - had to remove it when using books.csv, but also could have meant reading bookloans.csv a lot harder if prior information from the assignment brief hadn't been provided.\n",
    "\n",
    "- It was frustrating to work with some missing information re: book titles/authors. There is incomplete data obvious in the report where the authors are missing in a handful of titles.\n",
    "\n",
    "- using Microsoft Excel Epoch date format felt strange and clunky when looking at it for the first time but it was fairly straightforward to convert that information to intergers/floats to represent each individual loan period.\n",
    "\n",
    "- I wondered - are those who haven't returned their books back (date_of_return == 0) to be counted within the various statistics? Are those items considered as a late return, even if the book hasn't actually be returned yet? I have included both in Task 3, just in case.\n",
    "\n",
    "- In terms of the returned data, I used the .round() function to make the numbers a little more readable and usable (i.e. to two (2) decimal places)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
